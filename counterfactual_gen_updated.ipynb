{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae42a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Not supported in PyTorch < 2.0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    BertForMaskedLM, GPT2LMHeadModel, GPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations # Added for multi-token combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5975397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# !! IMPORTANT !! Change this to your own cache directory !!\n",
    "cache_dir = \"/ssd_scratch/sweta.jena\"  # <-- CHANGE THIS\n",
    "dataset_name=\"imdb\"\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "# sim_threshold=0.85\n",
    "# ppl_threshold=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5d97c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Model Loading\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# sentiment clf\n",
    "clf_name = \"textattack/bert-base-uncased-imdb\"    \n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(clf_name, cache_dir = cache_dir)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(clf_name, cache_dir=cache_dir)\n",
    "clf_model.eval()\n",
    "\n",
    "# masked LM\n",
    "mlm_name = \"bert-base-uncased\"                  \n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(mlm_name, cache_dir = cache_dir)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(mlm_name, cache_dir=cache_dir)\n",
    "mlm_model.eval()\n",
    "\n",
    "\n",
    "# fluency\n",
    "gpt2_name = \"gpt2\"                            \n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_name, cache_dir = cache_dir)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_name, cache_dir=cache_dir)\n",
    "gpt2_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    clf_model = torch.nn.DataParallel(clf_model)\n",
    "    mlm_model = torch.nn.DataParallel(mlm_model)\n",
    "    gpt2_model = torch.nn.DataParallel(gpt2_model)\n",
    "clf_model.to(device)\n",
    "mlm_model.to(device)\n",
    "gpt2_model.to(device)\n",
    "\n",
    "#semantic sim\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=str(device))\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b29c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def predict_proba(texts, batch_size=64):\n",
    "    \n",
    "    processed_texts = []\n",
    "    for t in texts:\n",
    "        if isinstance(t, (list, np.ndarray)):\n",
    "            processed_texts.append(clf_tokenizer.decode(t, skip_special_tokens=True))\n",
    "        else:\n",
    "            processed_texts.append(str(t))\n",
    "\n",
    "    all_probs = []\n",
    "    for i in range(0, len(processed_texts), batch_size):\n",
    "        batch = processed_texts[i:i+batch_size]\n",
    "        encodings = clf_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clf_model(**encodings)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).detach().cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "    return np.array(all_probs)\n",
    "\n",
    "def generate_candidates(text, word, top_k=10):\n",
    "    tokens = mlm_tokenizer.tokenize(text, max_length=512, truncation=True)\n",
    "    if word not in tokens:\n",
    "        # Try to find the word even if it's been tokenized differently by SHAP\n",
    "        # This is a fallback, but the first-subword-strategy in batch_evaluate is the main one\n",
    "        found = False\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok.startswith(word):\n",
    "                word = tok\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # print(f\"Warning: Token '{word}' not in tokenized text.\")\n",
    "            return []\n",
    "    \n",
    "    try:\n",
    "        idx = tokens.index(word)\n",
    "    except ValueError:\n",
    "        # print(f\"Warning: Token '{word}' not found despite check.\")\n",
    "        return []\n",
    "\n",
    "    tokens[idx] = mlm_tokenizer.mask_token\n",
    "    masked_text = mlm_tokenizer.convert_tokens_to_string(tokens)\n",
    "    inputs = mlm_tokenizer(masked_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = mlm_model(**inputs).logits\n",
    "    mask_index = torch.where(inputs[\"input_ids\"][0] == mlm_tokenizer.mask_token_id)[0].item()\n",
    "    probs = torch.softmax(logits[0, mask_index], dim=0)\n",
    "    top_tokens = torch.topk(probs, top_k).indices.tolist()\n",
    "    candidates = [mlm_tokenizer.decode([t]) for t in top_tokens]\n",
    "    return candidates\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    emb1 = sbert.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = sbert.encode(text2, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(emb1, emb2))\n",
    "\n",
    "def perplexity(text):\n",
    "\n",
    "    model_config = gpt2_model.module.config if hasattr(gpt2_model, \"module\") else gpt2_model.config\n",
    "    max_length = model_config.n_positions\n",
    "    encodings = gpt2_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "    stride = 512\n",
    "\n",
    "    lls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs.loss * trg_len\n",
    "        lls.append(log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    return float(ppl)\n",
    "\n",
    "def edit_distance(a, b):\n",
    "    return 1 - SequenceMatcher(None, a.split(), b.split()).ratio()\n",
    "\n",
    "# Placeholder function for Attribute Isolation Logic\n",
    "def check_attribute_isolation(text_a, text_b, isolation_classifier=None, isolation_tokenizer=None, isolation_label=None):\n",
    "    \"\"\"\n",
    "    Placeholder for attribute isolation logic.\n",
    "    In a real multi-attribute task (e.g., sentiment + topic), you would:\n",
    "    1. Load a separate classifier trained on the non-target attribute (e.g., a topic classifier).\n",
    "    2. Tokenize both text_a (original) and text_b (counterfactual).\n",
    "    3. Get predictions from the isolation_classifier for both texts.\n",
    "    4. Return True if the predictions are the same, False otherwise.\n",
    "    \n",
    "    Since the IMDb dataset is single-attribute (sentiment), we'll just return True.\n",
    "    \"\"\"\n",
    "    # Example logic you would implement:\n",
    "    # if isolation_classifier is not None:\n",
    "    #     inputs_a = isolation_tokenizer(text_a, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    #     inputs_b = isolation_tokenizer(text_b, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    #     with torch.no_grad():\n",
    "    #         pred_a = torch.argmax(isolation_classifier(**inputs_a).logits, dim=1).item()\n",
    "    #         pred_b = torch.argmax(isolation_classifier(**inputs_b).logits, dim=1).item()\n",
    "    #     return pred_a == pred_b\n",
    "    \n",
    "    # For this project using IMDb, we assume isolation is met.\n",
    "    return True\n",
    "\n",
    "def evaluate_counterfactual(cf, orig_text, method, sim_threshold=0.75, ppl_threshold=200):\n",
    "    if cf is None:\n",
    "        return {\n",
    "            \"success\": -1,\n",
    "            \"method\": method,\n",
    "            \"original_text\": orig_text,\n",
    "            \"counterfactual_text\": None,\n",
    "            \"changed_word\": None,\n",
    "            \"semantic_similarity\": None,\n",
    "            \"perplexity\": None,\n",
    "            \"edit_distance\": None,\n",
    "            \"original_embedding\": None,\n",
    "            \"counterfactual_embedding\": None,\n",
    "            \"mced\": None,\n",
    "            \"edit_type\": cf.get(\"edit_type\", \"single-token\") # track edit type\n",
    "        }\n",
    "\n",
    "    orig_emb = sbert.encode(orig_text, convert_to_tensor=False)\n",
    "    cf_emb = sbert.encode(cf.get(\"counterfactual_text\"), convert_to_tensor=False)\n",
    "    ed = edit_distance(orig_text, cf.get(\"counterfactual_text\"))\n",
    "    mced = ed / max(1, len(orig_text.split()))  # normalized edit distance\n",
    "\n",
    "    if (cf.get(\"semantic_similarity\") >= sim_threshold) and (cf.get(\"perplexity\") <= ppl_threshold):\n",
    "        success = 1\n",
    "    else:\n",
    "        success = 0\n",
    "\n",
    "    return {\n",
    "        \"success\": success,\n",
    "        \"method\": method,\n",
    "        \"original_text\": cf.get(\"original_text\", orig_text),\n",
    "        \"counterfactual_text\": cf.get(\"counterfactual_text\"),\n",
    "        \"changed_word\": cf.get(\"changed_word\"),\n",
    "        \"semantic_similarity\": cf.get(\"semantic_similarity\"),\n",
    "        \"perplexity\": cf.get(\"perplexity\"),\n",
    "        \"edit_distance\": ed,\n",
    "        \"original_embedding\": orig_emb.tolist(),      \n",
    "        \"counterfactual_embedding\": cf_emb.tolist(),\n",
    "        \"mced\": mced,\n",
    "        \"edit_type\": cf.get(\"edit_type\", \"single-token\") # track edit type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b43fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Main Evaluation Function\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def batch_evaluate(\n",
    "    sample_size=-1,\n",
    "    dataset_name=\"imdb\",\n",
    "    batch_size=64,\n",
    "    max_len=512,\n",
    "    lime_num_samples=500,\n",
    "    shap_max_evals=500,\n",
    "    multi_token_top_k=3 # Smaller top_k for multi-token combos to manage speed\n",
    "):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    test_data = dataset[\"test\"]\n",
    "\n",
    "    if sample_size == -1:\n",
    "        sample_size = len(test_data)\n",
    "        examples=list(test_data)\n",
    "        \n",
    "    else:\n",
    "        examples = random.sample(list(test_data), sample_size)\n",
    "\n",
    "    print(\"Sample size:\", sample_size)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    masker = shap.maskers.Text(tokenizer=clf_tokenizer)\n",
    "    model_config = clf_model.module.config if hasattr(clf_model, \"module\") else clf_model.config\n",
    "    labels = [model_config.id2label[i] for i in range(len(model_config.id2label))]\n",
    "    shap_explainer = shap.Explainer(predict_proba, masker, output_names=labels)\n",
    "\n",
    "    lime_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i:i+batch_size]\n",
    "        texts = []\n",
    "        for ex in batch:\n",
    "            enc = clf_tokenizer(\n",
    "                ex[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            truncated_text = clf_tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True)\n",
    "            texts.append(truncated_text)\n",
    "\n",
    "        \n",
    "        # SHAP counterfactuals ####################################\n",
    "        \n",
    "        try:\n",
    "            shap_values = shap_explainer(texts, max_evals=shap_max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP explanation failed for batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for j, text in enumerate(texts):\n",
    "            try:\n",
    "                pred_label = np.argmax(predict_proba([text])[0])\n",
    "                \n",
    "                vals = shap_values.values[j]\n",
    "                tokens = shap_values.data[j]\n",
    "\n",
    "                # handle multi-class output\n",
    "                if vals.ndim == 1:\n",
    "                    token_importances = vals\n",
    "                else:\n",
    "                    token_importances = vals[:, pred_label]\n",
    "\n",
    "                top_indices = np.argsort(np.abs(token_importances))[-5:]\n",
    "                influential_tokens_shap = [tokens[idx].strip() for idx in top_indices if len(tokens[idx].strip()) > 0]\n",
    "\n",
    "                # --- Start of Single-Token Edit Logic (Original) ---\n",
    "                cands_by_word_shap = {}\n",
    "                for target_word in influential_tokens_shap:\n",
    "                    subword = mlm_tokenizer.tokenize(target_word)\n",
    "                    if len(subword) == 0:\n",
    "                        cands_by_word_shap[target_word] = []\n",
    "                        continue\n",
    "                    subword = subword[0]\n",
    "                    candidates = generate_candidates(text, subword, top_k=10) # Original top_k\n",
    "                    cands_by_word_shap[target_word] = candidates\n",
    "                    \n",
    "                    for cand in candidates:\n",
    "                        new_text = text.replace(target_word, cand)\n",
    "                        new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                        \n",
    "                        # Added Attribute Isolation Check\n",
    "                        isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                        if new_pred != pred_label and isolated:\n",
    "                            sim = semantic_similarity(text, new_text)\n",
    "                            flu = perplexity(new_text)\n",
    "                            cf_shap = {\n",
    "                                \"original_text\": text,\n",
    "                                \"original_pred\": pred_label,\n",
    "                                \"counterfactual_text\": new_text,\n",
    "                                \"counterfactual_pred\": new_pred,\n",
    "                                \"changed_word\": (target_word, cand),\n",
    "                                \"semantic_similarity\": sim,\n",
    "                                \"perplexity\": flu,\n",
    "                                \"edit_type\": \"single-token\" # Added edit_type\n",
    "                            }\n",
    "                            metrics = evaluate_counterfactual(cf_shap, text, method=\"SHAP\")\n",
    "                            results.append(metrics)\n",
    "                # --- End of Single-Token Edit Logic ---\n",
    "\n",
    "                # --- NEW: Start of Multi-Token Edit (2-token) Logic ---\n",
    "                for word1, word2 in combinations(cands_by_word_shap.keys(), 2):\n",
    "                    # Use smaller k for combinations to avoid combinatorial explosion\n",
    "                    for cand1 in cands_by_word_shap[word1][:multi_token_top_k]:\n",
    "                        for cand2 in cands_by_word_shap[word2][:multi_token_top_k]:\n",
    "                            # Apply replaces sequentially.  \n",
    "                            new_text = text.replace(word1, cand1).replace(word2, cand2)\n",
    "                            new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                            isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                            if new_pred != pred_label and isolated:\n",
    "                                sim = semantic_similarity(text, new_text)\n",
    "                                flu = perplexity(new_text)\n",
    "                                cf_shap_multi = {\n",
    "                                    \"original_text\": text,\n",
    "                                    \"original_pred\": pred_label,\n",
    "                                    \"counterfactual_text\": new_text,\n",
    "                                    \"counterfactual_pred\": new_pred,\n",
    "                                    \"changed_word\": (f\"{word1}->{cand1}\", f\"{word2}->{cand2}\"),\n",
    "                                    \"semantic_similarity\": sim,\n",
    "                                    \"perplexity\": flu,\n",
    "                                    \"edit_type\": \"multi-token (2)\"\n",
    "                                }\n",
    "                                metrics = evaluate_counterfactual(cf_shap_multi, text, method=\"SHAP-Multi-2\")\n",
    "                                results.append(metrics)\n",
    "                # --- NEW: End of Multi-Token Edit (2-token) Logic ---\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {j} in SHAP batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        pd.DataFrame(results).to_csv(cache_dir+f\"/intermediate_counterfactuals_{i}_shap.csv\", index=False)\n",
    "\n",
    "        # LIME counterfactuals ####################################\n",
    "    \n",
    "        for text in texts:\n",
    "            try:\n",
    "                exp = lime_explainer.explain_instance(\n",
    "                    text,\n",
    "                    predict_proba,\n",
    "                    num_features=5,\n",
    "                    labels=[0, 1],\n",
    "                    num_samples=lime_num_samples\n",
    "                )\n",
    "\n",
    "                pred_label = np.argmax(predict_proba([text])[0])\n",
    "                influential_tokens_lime = [w for w, score in sorted(exp.as_list(label=pred_label), key=lambda x: abs(x[1]), reverse=True)][:5]\n",
    "                if not influential_tokens_lime:\n",
    "                    continue\n",
    "\n",
    "                # --- Start of Single-Token Edit Logic (Original) ---\n",
    "                cands_by_word_lime = {}\n",
    "                for target_word in influential_tokens_lime:\n",
    "                    subword = mlm_tokenizer.tokenize(target_word)\n",
    "                    if len(subword) == 0:\n",
    "                        cands_by_word_lime[target_word] = []\n",
    "                        continue\n",
    "                    subword = subword[0]\n",
    "                    candidates = generate_candidates(text, subword, top_k=10)\n",
    "                    cands_by_word_lime[target_word] = candidates\n",
    "\n",
    "                    for cand in candidates:\n",
    "                        new_text = text.replace(target_word, cand)\n",
    "                        new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                        \n",
    "                        # Added Attribute Isolation Check\n",
    "                        isolated = check_attribute_isolation(text, new_text)\n",
    "                        \n",
    "                        if new_pred != pred_label and isolated:\n",
    "                            sim = semantic_similarity(text, new_text)\n",
    "                            flu = perplexity(new_text)\n",
    "                            cf_lime = {\n",
    "                                \"original_text\": text,\n",
    "                                \"original_pred\": pred_label,\n",
    "                                \"counterfactual_text\": new_text,\n",
    "                                \"counterfactual_pred\": new_pred,\n",
    "                                \"changed_word\": (target_word, cand),\n",
    "                                \"semantic_similarity\": sim,\n",
    "                                \"perplexity\": flu,\n",
    "                                \"edit_type\": \"single-token\" # Added edit_type\n",
    "                            }\n",
    "                            metrics = evaluate_counterfactual(cf_lime, text, method=\"LIME\")\n",
    "                            results.append(metrics)\n",
    "                # --- End of Single-Token Edit Logic ---\n",
    "\n",
    "                # --- NEW: Start of Multi-Token Edit (2-token) Logic ---\n",
    "                for word1, word2 in combinations(cands_by_word_lime.keys(), 2):\n",
    "                    for cand1 in cands_by_word_lime[word1][:multi_token_top_k]:\n",
    "                        for cand2 in cands_by_word_lime[word2][:multi_token_top_k]:\n",
    "                            new_text = text.replace(word1, cand1).replace(word2, cand2)\n",
    "                            new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                            isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                            if new_pred != pred_label and isolated:\n",
    "                                sim = semantic_similarity(text, new_text)\n",
    "                                flu = perplexity(new_text)\n",
    "                                cf_lime_multi = {\n",
    "                                    \"original_text\": text,\n",
    "                                    \"original_pred\": pred_label,\n",
    "                                    \"counterfactual_text\": new_text,\n",
    "                                    \"counterfactual_pred\": new_pred,\n",
    "                                    \"changed_word\": (f\"{word1}->{cand1}\", f\"{word2}->{cand2}\"),\n",
    "                                    \"semantic_similarity\": sim,\n",
    "                                    \"perplexity\": flu,\n",
    "                                    \"edit_type\": \"multi-token (2)\"\n",
    "                                }\n",
    "                                metrics = evaluate_counterfactual(cf_lime_multi, text, method=\"LIME-Multi-2\")\n",
    "                                results.append(metrics)\n",
    "                # --- NEW: End of Multi-Token Edit (2-token) Logic ---\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text in LIME batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        pd.DataFrame(results).to_csv(cache_dir+f\"/intermediate_counterfactuals_{i}_shap_lime.csv\", index=False)\n",
    "        print(f\"Processed {min(i + batch_size, sample_size)}/{sample_size} examples\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"all_counterfactuals.csv\", index=False)\n",
    "\n",
    "    summary = df[df[\"success\"] == 1].groupby(\"method\").mean(numeric_only=True).to_dict()\n",
    "\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting counterfactual generation and evaluation...\n",
      "Sample size: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   6%|â–‹         | 4/64 [00:24<02:01,  2.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb9b8d67068479a96cb50f717e7f70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  11%|â–ˆ         | 7/64 [00:37<03:04,  3.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80dde2a5714243a7913ba26e8f2132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  22%|â–ˆâ–ˆâ–       | 14/64 [01:00<02:30,  3.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dee2eb0550546a6930ac0a5c8e7a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  30%|â–ˆâ–ˆâ–‰       | 19/64 [01:20<02:37,  3.50s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5020c04d1f67405fb807b707652bb206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [01:33<02:40,  3.82s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e12a9b85fc4968874bdb9df0926819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [01:43<02:47,  4.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b250de8ecb940c493990d0eae0bdb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [01:50<03:13,  4.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628b2414362645889daa67ae4e5291de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [02:20<01:51,  3.59s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5102d98482934280910dbc059cd6b8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/64 [02:37<01:41,  3.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5093db6111834212945e4d0884083dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 47/64 [03:14<00:55,  3.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9307cdb110134cafa510a2ff5200168c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 51/64 [03:26<00:34,  2.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4c5d076c61420da3eefc0e66a09f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [03:41<00:27,  3.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9405cf2b17e447fc9cf02753e03b14ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [03:48<00:35,  4.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4396c46985941f2a23ceed1457a015c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [03:56<00:37,  5.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8370f84b6aab450ebfd22814f855b5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [04:03<00:35,  5.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007f67d05168452799005d408a4c070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [04:10<00:31,  6.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b743cc11d71640cd86f4f9280d67360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [04:19<00:15,  5.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39270d1f576b49ed8a1e5364694e8ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 65it [04:32,  4.26s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64/25000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883703bda85a4be89e7f0a018cec1e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   9%|â–‰         | 6/64 [00:14<01:11,  1.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d2263dff35499a977033a249466a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  12%|â–ˆâ–Ž        | 8/64 [00:26<03:54,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969ad4cc069343c58a8630682904e7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  16%|â–ˆâ–Œ        | 10/64 [00:35<03:53,  4.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f89895ec5447158a6ceea67f275dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  17%|â–ˆâ–‹        | 11/64 [00:43<04:40,  5.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b57215f3f5453f9b9925028eaebc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  20%|â–ˆâ–ˆ        | 13/64 [00:55<04:38,  5.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add9645f002543e9b080a82d01dbb577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [01:05<04:10,  5.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d03df29be0a4072b8e6284aeac3f258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [01:12<04:30,  5.64s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d477f029b24616b9a5fefcb61cb8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  28%|â–ˆâ–ˆâ–Š       | 18/64 [01:21<03:49,  4.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1b3e1225147bc88165adb6a67ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [01:41<02:48,  4.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807854a2a3fe471fa508e603245739f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [01:53<03:03,  4.70s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e86d2f0d634513a2f9ddce470b78d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [02:15<02:12,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01061688cecd4b44aa8caee5be605a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [02:36<01:52,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6332b4af20d24096842308a62e21f3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/64 [02:50<01:14,  2.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e146545c5441fa1de60f4cc884804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/64 [03:14<00:53,  2.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22269a23d16425394930f5988d7753d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [03:27<00:52,  3.51s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e973422a9f4587b2a5a17cd2cf702f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/64 [03:48<00:38,  3.83s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2babb56b963422e918452e448a678f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [03:55<00:44,  4.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53b5d75be104a8e92267a1987814f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [04:10<00:28,  4.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b056f45b42a041bfb2d1fb287f3764f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [04:17<00:26,  5.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e3c0c08f1c442d87fe7a64f6600f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 65it [04:41,  4.61s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128/25000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1544a7071345f7ab4d56f2fd4160c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dff02875f0540e4a260a291e07e7360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   5%|â–         | 3/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd39c31f9eb4b0fa6e47f824f821bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  11%|â–ˆ         | 7/64 [00:31<03:41,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0943d8896ca4e039403e8e343da8bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  16%|â–ˆâ–Œ        | 10/64 [00:42<03:10,  3.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998a17f3d89741028bd90f66371da063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  17%|â–ˆâ–‹        | 11/64 [00:49<04:12,  4.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b211506d1a0744249aaac100879c5518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  20%|â–ˆâ–ˆ        | 13/64 [01:00<04:13,  4.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ef11bf29114714ac5b4fd5acad8ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  28%|â–ˆâ–ˆâ–Š       | 18/64 [01:18<02:26,  3.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160ab3a2feef4d8b94b8716566fab2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [01:34<01:30,  2.27s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3594a97bcc4b12bfbd98f931607d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [01:51<01:19,  2.33s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60d44240384982b200f483a1ec6966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [02:03<01:39,  3.22s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ecdaef43d742808568b0f905b4676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/64 [02:33<01:23,  3.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee2f57548a24e3c9bf5f980e48ca4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [03:28<00:13,  2.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3a7721bc041a49ef105559591a150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 65it [03:53,  3.77s/it]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 192/25000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be52368bedf040d0a411f9b67a62d866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   5%|â–         | 3/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7697abfa8f47b885daa6583045d495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  12%|â–ˆâ–Ž        | 8/64 [00:26<02:29,  2.67s/it]"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Script Execution\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting counterfactual generation and evaluation...\")\n",
    "    \n",
    "    # To run on a small sample for testing, change sample_size:\n",
    "    # df, summary = batch_evaluate(sample_size=10, dataset_name=dataset_name)\n",
    "    \n",
    "    # To run on the full dataset (as in the notebook):\n",
    "    df, summary = batch_evaluate(sample_size=-1, dataset_name=dataset_name)\n",
    "    \n",
    "    print(\"\\n--- Evaluation Summary (Successful Counterfactuals) ---\")\n",
    "    print(summary)\n",
    "    print(\"\\nProcess complete. Results saved to 'all_counterfactuals.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_storysumm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
