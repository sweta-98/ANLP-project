{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae42a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hardik.mittal/miniconda3/envs/new_storysumm/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Not supported in PyTorch < 2.0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    BertForMaskedLM, GPT2LMHeadModel, GPT2TokenizerFast\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations # Added for multi-token combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5975397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# !! IMPORTANT !! Change this to your own cache directory !!\n",
    "cache_dir = \"/ssd_scratch/sweta.jena\"  # <-- CHANGE THIS\n",
    "dataset_name=\"imdb\"\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "# sim_threshold=0.85\n",
    "# ppl_threshold=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5d97c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Model Loading\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# sentiment clf\n",
    "clf_name = \"textattack/bert-base-uncased-imdb\"    \n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(clf_name, cache_dir = cache_dir)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(clf_name, cache_dir=cache_dir)\n",
    "clf_model.eval()\n",
    "\n",
    "# masked LM\n",
    "mlm_name = \"bert-base-uncased\"                  \n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(mlm_name, cache_dir = cache_dir)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(mlm_name, cache_dir=cache_dir)\n",
    "mlm_model.eval()\n",
    "\n",
    "\n",
    "# fluency\n",
    "gpt2_name = \"gpt2\"                            \n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(gpt2_name, cache_dir = cache_dir)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_name, cache_dir=cache_dir)\n",
    "gpt2_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    clf_model = torch.nn.DataParallel(clf_model)\n",
    "    mlm_model = torch.nn.DataParallel(mlm_model)\n",
    "    gpt2_model = torch.nn.DataParallel(gpt2_model)\n",
    "clf_model.to(device)\n",
    "mlm_model.to(device)\n",
    "gpt2_model.to(device)\n",
    "\n",
    "#semantic sim\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\", device=str(device))\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b29c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def predict_proba(texts, batch_size=64):\n",
    "    \n",
    "    processed_texts = []\n",
    "    for t in texts:\n",
    "        if isinstance(t, (list, np.ndarray)):\n",
    "            processed_texts.append(clf_tokenizer.decode(t, skip_special_tokens=True))\n",
    "        else:\n",
    "            processed_texts.append(str(t))\n",
    "\n",
    "    all_probs = []\n",
    "    for i in range(0, len(processed_texts), batch_size):\n",
    "        batch = processed_texts[i:i+batch_size]\n",
    "        encodings = clf_tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clf_model(**encodings)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).detach().cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "    return np.array(all_probs)\n",
    "\n",
    "def generate_candidates(text, word, top_k=10):\n",
    "    tokens = mlm_tokenizer.tokenize(text, max_length=512, truncation=True)\n",
    "    if word not in tokens:\n",
    "        # Try to find the word even if it's been tokenized differently by SHAP\n",
    "        # This is a fallback, but the first-subword-strategy in batch_evaluate is the main one\n",
    "        found = False\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok.startswith(word):\n",
    "                word = tok\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # print(f\"Warning: Token '{word}' not in tokenized text.\")\n",
    "            return []\n",
    "    \n",
    "    try:\n",
    "        idx = tokens.index(word)\n",
    "    except ValueError:\n",
    "        # print(f\"Warning: Token '{word}' not found despite check.\")\n",
    "        return []\n",
    "\n",
    "    tokens[idx] = mlm_tokenizer.mask_token\n",
    "    masked_text = mlm_tokenizer.convert_tokens_to_string(tokens)\n",
    "    inputs = mlm_tokenizer(masked_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = mlm_model(**inputs).logits\n",
    "    mask_index = torch.where(inputs[\"input_ids\"][0] == mlm_tokenizer.mask_token_id)[0].item()\n",
    "    probs = torch.softmax(logits[0, mask_index], dim=0)\n",
    "    top_tokens = torch.topk(probs, top_k).indices.tolist()\n",
    "    candidates = [mlm_tokenizer.decode([t]) for t in top_tokens]\n",
    "    return candidates\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    emb1 = sbert.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = sbert.encode(text2, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(emb1, emb2))\n",
    "\n",
    "def perplexity(text):\n",
    "\n",
    "    model_config = gpt2_model.module.config if hasattr(gpt2_model, \"module\") else gpt2_model.config\n",
    "    max_length = model_config.n_positions\n",
    "    encodings = gpt2_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "    stride = 512\n",
    "\n",
    "    lls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = gpt2_model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs.loss * trg_len\n",
    "        lls.append(log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    return float(ppl)\n",
    "\n",
    "def edit_distance(a, b):\n",
    "    return 1 - SequenceMatcher(None, a.split(), b.split()).ratio()\n",
    "\n",
    "# Placeholder function for Attribute Isolation Logic\n",
    "def check_attribute_isolation(text_a, text_b, isolation_classifier=None, isolation_tokenizer=None, isolation_label=None):\n",
    "    \"\"\"\n",
    "    Placeholder for attribute isolation logic.\n",
    "    In a real multi-attribute task (e.g., sentiment + topic), you would:\n",
    "    1. Load a separate classifier trained on the non-target attribute (e.g., a topic classifier).\n",
    "    2. Tokenize both text_a (original) and text_b (counterfactual).\n",
    "    3. Get predictions from the isolation_classifier for both texts.\n",
    "    4. Return True if the predictions are the same, False otherwise.\n",
    "    \n",
    "    Since the IMDb dataset is single-attribute (sentiment), we'll just return True.\n",
    "    \"\"\"\n",
    "    # Example logic you would implement:\n",
    "    # if isolation_classifier is not None:\n",
    "    #     inputs_a = isolation_tokenizer(text_a, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    #     inputs_b = isolation_tokenizer(text_b, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    #     with torch.no_grad():\n",
    "    #         pred_a = torch.argmax(isolation_classifier(**inputs_a).logits, dim=1).item()\n",
    "    #         pred_b = torch.argmax(isolation_classifier(**inputs_b).logits, dim=1).item()\n",
    "    #     return pred_a == pred_b\n",
    "    \n",
    "    # For this project using IMDb, we assume isolation is met.\n",
    "    return True\n",
    "\n",
    "def evaluate_counterfactual(cf, orig_text, method, sim_threshold=0.75, ppl_threshold=200):\n",
    "    if cf is None:\n",
    "        return {\n",
    "            \"success\": -1,\n",
    "            \"method\": method,\n",
    "            \"original_text\": orig_text,\n",
    "            \"counterfactual_text\": None,\n",
    "            \"changed_word\": None,\n",
    "            \"semantic_similarity\": None,\n",
    "            \"perplexity\": None,\n",
    "            \"edit_distance\": None,\n",
    "            \"original_embedding\": None,\n",
    "            \"counterfactual_embedding\": None,\n",
    "            \"mced\": None,\n",
    "            \"edit_type\": cf.get(\"edit_type\", \"single-token\") # track edit type\n",
    "        }\n",
    "\n",
    "    orig_emb = sbert.encode(orig_text, convert_to_tensor=False)\n",
    "    cf_emb = sbert.encode(cf.get(\"counterfactual_text\"), convert_to_tensor=False)\n",
    "    ed = edit_distance(orig_text, cf.get(\"counterfactual_text\"))\n",
    "    mced = ed / max(1, len(orig_text.split()))  # normalized edit distance\n",
    "\n",
    "    if (cf.get(\"semantic_similarity\") >= sim_threshold) and (cf.get(\"perplexity\") <= ppl_threshold):\n",
    "        success = 1\n",
    "    else:\n",
    "        success = 0\n",
    "\n",
    "    return {\n",
    "        \"success\": success,\n",
    "        \"method\": method,\n",
    "        \"original_text\": cf.get(\"original_text\", orig_text),\n",
    "        \"counterfactual_text\": cf.get(\"counterfactual_text\"),\n",
    "        \"changed_word\": cf.get(\"changed_word\"),\n",
    "        \"semantic_similarity\": cf.get(\"semantic_similarity\"),\n",
    "        \"perplexity\": cf.get(\"perplexity\"),\n",
    "        \"edit_distance\": ed,\n",
    "        \"original_embedding\": orig_emb.tolist(),      \n",
    "        \"counterfactual_embedding\": cf_emb.tolist(),\n",
    "        \"mced\": mced,\n",
    "        \"edit_type\": cf.get(\"edit_type\", \"single-token\") # track edit type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b43fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Main Evaluation Function\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def batch_evaluate(\n",
    "    sample_size=-1,\n",
    "    dataset_name=\"imdb\",\n",
    "    batch_size=64,\n",
    "    max_len=512,\n",
    "    lime_num_samples=500,\n",
    "    shap_max_evals=500,\n",
    "    multi_token_top_k=3 # Smaller top_k for multi-token combos to manage speed\n",
    "):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    test_data = dataset[\"test\"]\n",
    "\n",
    "    if sample_size == -1:\n",
    "        sample_size = len(test_data)\n",
    "        examples=list(test_data)\n",
    "        \n",
    "    else:\n",
    "        examples = random.sample(list(test_data), sample_size)\n",
    "\n",
    "    print(\"Sample size:\", sample_size)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    masker = shap.maskers.Text(tokenizer=clf_tokenizer)\n",
    "    model_config = clf_model.module.config if hasattr(clf_model, \"module\") else clf_model.config\n",
    "    labels = [model_config.id2label[i] for i in range(len(model_config.id2label))]\n",
    "    shap_explainer = shap.Explainer(predict_proba, masker, output_names=labels)\n",
    "\n",
    "    lime_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i:i+batch_size]\n",
    "        texts = []\n",
    "        for ex in batch:\n",
    "            enc = clf_tokenizer(\n",
    "                ex[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=False,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            truncated_text = clf_tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True)\n",
    "            texts.append(truncated_text)\n",
    "\n",
    "        \n",
    "        # SHAP counterfactuals ####################################\n",
    "        \n",
    "        try:\n",
    "            shap_values = shap_explainer(texts, max_evals=shap_max_evals)\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP explanation failed for batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for j, text in enumerate(texts):\n",
    "            try:\n",
    "                pred_label = np.argmax(predict_proba([text])[0])\n",
    "                \n",
    "                vals = shap_values.values[j]\n",
    "                tokens = shap_values.data[j]\n",
    "\n",
    "                # handle multi-class output\n",
    "                if vals.ndim == 1:\n",
    "                    token_importances = vals\n",
    "                else:\n",
    "                    token_importances = vals[:, pred_label]\n",
    "\n",
    "                top_indices = np.argsort(np.abs(token_importances))[-5:]\n",
    "                influential_tokens_shap = [tokens[idx].strip() for idx in top_indices if len(tokens[idx].strip()) > 0]\n",
    "\n",
    "                # --- Start of Single-Token Edit Logic (Original) ---\n",
    "                cands_by_word_shap = {}\n",
    "                for target_word in influential_tokens_shap:\n",
    "                    subword = mlm_tokenizer.tokenize(target_word)\n",
    "                    if len(subword) == 0:\n",
    "                        cands_by_word_shap[target_word] = []\n",
    "                        continue\n",
    "                    subword = subword[0]\n",
    "                    candidates = generate_candidates(text, subword, top_k=10) # Original top_k\n",
    "                    cands_by_word_shap[target_word] = candidates\n",
    "                    \n",
    "                    for cand in candidates:\n",
    "                        new_text = text.replace(target_word, cand)\n",
    "                        new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                        \n",
    "                        # Added Attribute Isolation Check\n",
    "                        isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                        if new_pred != pred_label and isolated:\n",
    "                            sim = semantic_similarity(text, new_text)\n",
    "                            flu = perplexity(new_text)\n",
    "                            cf_shap = {\n",
    "                                \"original_text\": text,\n",
    "                                \"original_pred\": pred_label,\n",
    "                                \"counterfactual_text\": new_text,\n",
    "                                \"counterfactual_pred\": new_pred,\n",
    "                                \"changed_word\": (target_word, cand),\n",
    "                                \"semantic_similarity\": sim,\n",
    "                                \"perplexity\": flu,\n",
    "                                \"edit_type\": \"single-token\" # Added edit_type\n",
    "                            }\n",
    "                            metrics = evaluate_counterfactual(cf_shap, text, method=\"SHAP\")\n",
    "                            results.append(metrics)\n",
    "                # --- End of Single-Token Edit Logic ---\n",
    "\n",
    "                # --- NEW: Start of Multi-Token Edit (2-token) Logic ---\n",
    "                for word1, word2 in combinations(cands_by_word_shap.keys(), 2):\n",
    "                    # Use smaller k for combinations to avoid combinatorial explosion\n",
    "                    for cand1 in cands_by_word_shap[word1][:multi_token_top_k]:\n",
    "                        for cand2 in cands_by_word_shap[word2][:multi_token_top_k]:\n",
    "                            # Apply replaces sequentially.  \n",
    "                            new_text = text.replace(word1, cand1).replace(word2, cand2)\n",
    "                            new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                            isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                            if new_pred != pred_label and isolated:\n",
    "                                sim = semantic_similarity(text, new_text)\n",
    "                                flu = perplexity(new_text)\n",
    "                                cf_shap_multi = {\n",
    "                                    \"original_text\": text,\n",
    "                                    \"original_pred\": pred_label,\n",
    "                                    \"counterfactual_text\": new_text,\n",
    "                                    \"counterfactual_pred\": new_pred,\n",
    "                                    \"changed_word\": (f\"{word1}->{cand1}\", f\"{word2}->{cand2}\"),\n",
    "                                    \"semantic_similarity\": sim,\n",
    "                                    \"perplexity\": flu,\n",
    "                                    \"edit_type\": \"multi-token (2)\"\n",
    "                                }\n",
    "                                metrics = evaluate_counterfactual(cf_shap_multi, text, method=\"SHAP-Multi-2\")\n",
    "                                results.append(metrics)\n",
    "                # --- NEW: End of Multi-Token Edit (2-token) Logic ---\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {j} in SHAP batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        pd.DataFrame(results).to_csv(cache_dir+f\"/intermediate_counterfactuals_{i}_shap.csv\", index=False)\n",
    "\n",
    "        # LIME counterfactuals ####################################\n",
    "    \n",
    "        for text in texts:\n",
    "            try:\n",
    "                exp = lime_explainer.explain_instance(\n",
    "                    text,\n",
    "                    predict_proba,\n",
    "                    num_features=5,\n",
    "                    labels=[0, 1],\n",
    "                    num_samples=lime_num_samples\n",
    "                )\n",
    "\n",
    "                pred_label = np.argmax(predict_proba([text])[0])\n",
    "                influential_tokens_lime = [w for w, score in sorted(exp.as_list(label=pred_label), key=lambda x: abs(x[1]), reverse=True)][:5]\n",
    "                if not influential_tokens_lime:\n",
    "                    continue\n",
    "\n",
    "                # --- Start of Single-Token Edit Logic (Original) ---\n",
    "                cands_by_word_lime = {}\n",
    "                for target_word in influential_tokens_lime:\n",
    "                    subword = mlm_tokenizer.tokenize(target_word)\n",
    "                    if len(subword) == 0:\n",
    "                        cands_by_word_lime[target_word] = []\n",
    "                        continue\n",
    "                    subword = subword[0]\n",
    "                    candidates = generate_candidates(text, subword, top_k=10)\n",
    "                    cands_by_word_lime[target_word] = candidates\n",
    "\n",
    "                    for cand in candidates:\n",
    "                        new_text = text.replace(target_word, cand)\n",
    "                        new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                        \n",
    "                        # Added Attribute Isolation Check\n",
    "                        isolated = check_attribute_isolation(text, new_text)\n",
    "                        \n",
    "                        if new_pred != pred_label and isolated:\n",
    "                            sim = semantic_similarity(text, new_text)\n",
    "                            flu = perplexity(new_text)\n",
    "                            cf_lime = {\n",
    "                                \"original_text\": text,\n",
    "                                \"original_pred\": pred_label,\n",
    "                                \"counterfactual_text\": new_text,\n",
    "                                \"counterfactual_pred\": new_pred,\n",
    "                                \"changed_word\": (target_word, cand),\n",
    "                                \"semantic_similarity\": sim,\n",
    "                                \"perplexity\": flu,\n",
    "                                \"edit_type\": \"single-token\" # Added edit_type\n",
    "                            }\n",
    "                            metrics = evaluate_counterfactual(cf_lime, text, method=\"LIME\")\n",
    "                            results.append(metrics)\n",
    "                # --- End of Single-Token Edit Logic ---\n",
    "\n",
    "                # --- NEW: Start of Multi-Token Edit (2-token) Logic ---\n",
    "                for word1, word2 in combinations(cands_by_word_lime.keys(), 2):\n",
    "                    for cand1 in cands_by_word_lime[word1][:multi_token_top_k]:\n",
    "                        for cand2 in cands_by_word_lime[word2][:multi_token_top_k]:\n",
    "                            new_text = text.replace(word1, cand1).replace(word2, cand2)\n",
    "                            new_pred = np.argmax(predict_proba([new_text])[0])\n",
    "                            isolated = check_attribute_isolation(text, new_text)\n",
    "\n",
    "                            if new_pred != pred_label and isolated:\n",
    "                                sim = semantic_similarity(text, new_text)\n",
    "                                flu = perplexity(new_text)\n",
    "                                cf_lime_multi = {\n",
    "                                    \"original_text\": text,\n",
    "                                    \"original_pred\": pred_label,\n",
    "                                    \"counterfactual_text\": new_text,\n",
    "                                    \"counterfactual_pred\": new_pred,\n",
    "                                    \"changed_word\": (f\"{word1}->{cand1}\", f\"{word2}->{cand2}\"),\n",
    "                                    \"semantic_similarity\": sim,\n",
    "                                    \"perplexity\": flu,\n",
    "                                    \"edit_type\": \"multi-token (2)\"\n",
    "                                }\n",
    "                                metrics = evaluate_counterfactual(cf_lime_multi, text, method=\"LIME-Multi-2\")\n",
    "                                results.append(metrics)\n",
    "                # --- NEW: End of Multi-Token Edit (2-token) Logic ---\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text in LIME batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        pd.DataFrame(results).to_csv(cache_dir+f\"/intermediate_counterfactuals_{i}_shap_lime.csv\", index=False)\n",
    "        print(f\"Processed {min(i + batch_size, sample_size)}/{sample_size} examples\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"all_counterfactuals.csv\", index=False)\n",
    "\n",
    "    summary = df[df[\"success\"] == 1].groupby(\"method\").mean(numeric_only=True).to_dict()\n",
    "\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting counterfactual generation and evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   6%|â–‹         | 4/64 [00:14<01:04,  1.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7959eed91d004fbd8b1975b662202e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  11%|â–ˆ         | 7/64 [00:26<02:53,  3.04s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b93597325f84d0998b6515fe37a2780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  22%|â–ˆâ–ˆâ–       | 14/64 [00:49<02:28,  2.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4ae7ebda1045c98ebcbf50394dd2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [01:22<02:37,  3.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e77038ce5147c49b2a12822ee6aabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [01:32<02:45,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911ce34ad1924877ab15e4d10e6df5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [01:38<03:11,  4.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dd42e8a801463598c593fc12302ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [02:08<01:50,  3.55s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cadd29b3e44eaabea6d056115bd0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/64 [02:25<01:40,  3.72s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb4e70192348488661d891b720f584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 47/64 [03:02<00:54,  3.20s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d934d781e244946bbfb1ae8817ff272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 51/64 [03:14<00:34,  2.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f1af9eac494c24967fca3fe79148c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [03:29<00:27,  3.08s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5dce9f38634e8aa0aa5e2a5b76187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [03:36<00:34,  4.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da276a4a834747d4abcdc7a71131d948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [03:43<00:36,  5.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1033132782e14aa08cc59677519b9c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [03:51<00:35,  5.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2d697a19204f30a5896ceeaa0257a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [03:58<00:31,  6.33s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcc40c5137a4a98ba0f40ea4f25cebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [04:06<00:15,  5.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d020dc893284d119a7500dd555aca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 65it [04:20,  4.13s/it]                        \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64/25000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e87077cc4a641d383b1afa7356b35cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:   9%|â–‰         | 6/64 [00:14<01:12,  1.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d67569e320e4c2cb9d14aa51e5a65d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  12%|â–ˆâ–Ž        | 8/64 [00:26<03:53,  4.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2042a4dbbc4150a55bb7ae8ec82fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  16%|â–ˆâ–Œ        | 10/64 [00:35<03:51,  4.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2c7bf8d9b484081a45821c02aa6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  17%|â–ˆâ–‹        | 11/64 [00:42<04:38,  5.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7fe9e19b13431792ceef2261f17ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  20%|â–ˆâ–ˆ        | 13/64 [00:54<04:36,  5.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b5922cf5e844189f57632d9c2e7f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [01:04<04:09,  5.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397df469859b4fdcbc307b0c0d9a7270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [01:11<04:28,  5.59s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d207ce836b7049a8882543edaca671c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  28%|â–ˆâ–ˆâ–Š       | 18/64 [01:21<03:47,  4.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8520c75d8ef14271b875dde80929491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [01:41<02:46,  4.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea163872fe09478e9cfe7e386ffe499c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [01:52<03:01,  4.66s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1ce6726365410d91204e90db842775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [02:14<02:11,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eea02e1d1b34d7293c650169e2dcbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [02:35<01:51,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208de12481ce4520a33d7c124fbc2b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/64 [02:48<01:13,  2.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ac09f4baa748af8125489a2c76919e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/64 [03:12<00:53,  2.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3b87f02f8a42b88bf4bd66779b2a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [03:25<00:51,  3.46s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556365dafa9f4e2e804a735c3fc0d579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/64 [03:46<00:37,  3.77s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204f2b866e9e404baf80de9072ba565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [03:53<00:43,  4.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6110a9247b84da3be4dd1fff2941b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [04:08<00:28,  4.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf1b565f0b1403b92f23f8ac870b09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [04:14<00:25,  5.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b33a3db95bc43e8a264279fe9e8fd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partition explainer: 65it [04:38,  4.56s/it]                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Script Execution\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting counterfactual generation and evaluation...\")\n",
    "    \n",
    "    # To run on a small sample for testing, change sample_size:\n",
    "    # df, summary = batch_evaluate(sample_size=10, dataset_name=dataset_name)\n",
    "    \n",
    "    # To run on the full dataset (as in the notebook):\n",
    "    df, summary = batch_evaluate(sample_size=-1, dataset_name=dataset_name)\n",
    "    \n",
    "    print(\"\\n--- Evaluation Summary (Successful Counterfactuals) ---\")\n",
    "    print(summary)\n",
    "    print(\"\\nProcess complete. Results saved to 'all_counterfactuals.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feefdb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /ssd_scratch/sweta.jena/intermediate_counterfactuals_896_shap.csv\n",
      "Total rows: 11903\n",
      "Columns: ['success', 'method', 'original_text', 'counterfactual_text', 'changed_word', 'semantic_similarity', 'perplexity', 'edit_distance', 'original_embedding', 'counterfactual_embedding', 'mced', 'edit_type']\n",
      "\n",
      "================================================================================\n",
      "First 10 rows:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "      <th>method</th>\n",
       "      <th>original_text</th>\n",
       "      <th>counterfactual_text</th>\n",
       "      <th>changed_word</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>edit_distance</th>\n",
       "      <th>original_embedding</th>\n",
       "      <th>counterfactual_embedding</th>\n",
       "      <th>mced</th>\n",
       "      <th>edit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>('.', 'but')</td>\n",
       "      <td>0.970543</td>\n",
       "      <td>68.212982</td>\n",
       "      <td>0.054393</td>\n",
       "      <td>[0.013422119431197643, -0.05341522768139839, 0...</td>\n",
       "      <td>[0.01939862035214901, -0.0657210573554039, -0....</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>('.', 'and')</td>\n",
       "      <td>0.978007</td>\n",
       "      <td>73.863052</td>\n",
       "      <td>0.054393</td>\n",
       "      <td>[0.013422119431197643, -0.05341522768139839, 0...</td>\n",
       "      <td>[0.015377247706055641, -0.07202999293804169, 0...</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP-Multi-2</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>worth the entertainment value of a rental, esp...</td>\n",
       "      <td>('.-&gt;...', 'not-&gt;very')</td>\n",
       "      <td>0.938516</td>\n",
       "      <td>58.596718</td>\n",
       "      <td>0.104603</td>\n",
       "      <td>[0.013422119431197643, -0.05341522768139839, 0...</td>\n",
       "      <td>[0.00359835266135633, -0.054686449468135834, 0...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>multi-token (2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'is')</td>\n",
       "      <td>0.996635</td>\n",
       "      <td>58.650810</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.05127647519111633, 0.02394253946840763, -0...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'was')</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>59.027637</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.049222689121961594, 0.028409535065293312, ...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'bit')</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>62.394833</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.04825850576162338, 0.03049815073609352, -0...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'its')</td>\n",
       "      <td>0.999431</td>\n",
       "      <td>59.807793</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.04866982251405716, 0.02756422385573387, -0...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'it')</td>\n",
       "      <td>0.999312</td>\n",
       "      <td>60.539921</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.048071280121803284, 0.02763962559401989, -...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('not', 'but')</td>\n",
       "      <td>0.999286</td>\n",
       "      <td>60.959511</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.050051577389240265, 0.029488245025277138, ...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>single-token</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>SHAP-Multi-2</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>its a totally average film with a few semi - a...</td>\n",
       "      <td>('little-&gt;lot', 'not-&gt;is')</td>\n",
       "      <td>0.995233</td>\n",
       "      <td>65.385201</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>[-0.04979102686047554, 0.030777275562286377, -...</td>\n",
       "      <td>[-0.05124744772911072, 0.021253619343042374, -...</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>multi-token (2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   success        method                                      original_text  \\\n",
       "0        1          SHAP  worth the entertainment value of a rental, esp...   \n",
       "1        1          SHAP  worth the entertainment value of a rental, esp...   \n",
       "2        1  SHAP-Multi-2  worth the entertainment value of a rental, esp...   \n",
       "3        1          SHAP  its a totally average film with a few semi - a...   \n",
       "4        1          SHAP  its a totally average film with a few semi - a...   \n",
       "5        1          SHAP  its a totally average film with a few semi - a...   \n",
       "6        1          SHAP  its a totally average film with a few semi - a...   \n",
       "7        1          SHAP  its a totally average film with a few semi - a...   \n",
       "8        1          SHAP  its a totally average film with a few semi - a...   \n",
       "9        1  SHAP-Multi-2  its a totally average film with a few semi - a...   \n",
       "\n",
       "                                 counterfactual_text  \\\n",
       "0  worth the entertainment value of a rental, esp...   \n",
       "1  worth the entertainment value of a rental, esp...   \n",
       "2  worth the entertainment value of a rental, esp...   \n",
       "3  its a totally average film with a few semi - a...   \n",
       "4  its a totally average film with a few semi - a...   \n",
       "5  its a totally average film with a few semi - a...   \n",
       "6  its a totally average film with a few semi - a...   \n",
       "7  its a totally average film with a few semi - a...   \n",
       "8  its a totally average film with a few semi - a...   \n",
       "9  its a totally average film with a few semi - a...   \n",
       "\n",
       "                 changed_word  semantic_similarity  perplexity  edit_distance  \\\n",
       "0                ('.', 'but')             0.970543   68.212982       0.054393   \n",
       "1                ('.', 'and')             0.978007   73.863052       0.054393   \n",
       "2     ('.->...', 'not->very')             0.938516   58.596718       0.104603   \n",
       "3               ('not', 'is')             0.996635   58.650810       0.007407   \n",
       "4              ('not', 'was')             0.999207   59.027637       0.007407   \n",
       "5              ('not', 'bit')             0.999036   62.394833       0.007407   \n",
       "6              ('not', 'its')             0.999431   59.807793       0.007407   \n",
       "7               ('not', 'it')             0.999312   60.539921       0.007407   \n",
       "8              ('not', 'but')             0.999286   60.959511       0.007407   \n",
       "9  ('little->lot', 'not->is')             0.995233   65.385201       0.022222   \n",
       "\n",
       "                                  original_embedding  \\\n",
       "0  [0.013422119431197643, -0.05341522768139839, 0...   \n",
       "1  [0.013422119431197643, -0.05341522768139839, 0...   \n",
       "2  [0.013422119431197643, -0.05341522768139839, 0...   \n",
       "3  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "4  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "5  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "6  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "7  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "8  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "9  [-0.04979102686047554, 0.030777275562286377, -...   \n",
       "\n",
       "                            counterfactual_embedding      mced  \\\n",
       "0  [0.01939862035214901, -0.0657210573554039, -0....  0.000228   \n",
       "1  [0.015377247706055641, -0.07202999293804169, 0...  0.000228   \n",
       "2  [0.00359835266135633, -0.054686449468135834, 0...  0.000438   \n",
       "3  [-0.05127647519111633, 0.02394253946840763, -0...  0.000055   \n",
       "4  [-0.049222689121961594, 0.028409535065293312, ...  0.000055   \n",
       "5  [-0.04825850576162338, 0.03049815073609352, -0...  0.000055   \n",
       "6  [-0.04866982251405716, 0.02756422385573387, -0...  0.000055   \n",
       "7  [-0.048071280121803284, 0.02763962559401989, -...  0.000055   \n",
       "8  [-0.050051577389240265, 0.029488245025277138, ...  0.000055   \n",
       "9  [-0.05124744772911072, 0.021253619343042374, -...  0.000165   \n",
       "\n",
       "         edit_type  \n",
       "0     single-token  \n",
       "1     single-token  \n",
       "2  multi-token (2)  \n",
       "3     single-token  \n",
       "4     single-token  \n",
       "5     single-token  \n",
       "6     single-token  \n",
       "7     single-token  \n",
       "8     single-token  \n",
       "9  multi-token (2)  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and display the latest intermediate results\n",
    "import pandas as pd\n",
    "\n",
    "latest_file = \"/ssd_scratch/sweta.jena/intermediate_counterfactuals_896_shap.csv\"\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "print(f\"File: {latest_file}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"First 10 rows:\")\n",
    "print(\"=\"*80)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0df7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total counterfactuals generated: 11903\n",
      "Successful counterfactuals: 10506\n",
      "Success rate: 88.26%\n",
      "\n",
      "By Method:\n",
      "              count   sum      mean\n",
      "method                             \n",
      "LIME           1551  1408  0.907801\n",
      "LIME-Multi-2   4773  4361  0.913681\n",
      "SHAP           1496  1280  0.855615\n",
      "SHAP-Multi-2   4083  3457  0.846681\n",
      "\n",
      "By Edit Type:\n",
      "                 count   sum      mean\n",
      "edit_type                             \n",
      "multi-token (2)   8856  7818  0.882791\n",
      "single-token      3047  2688  0.882179\n",
      "\n",
      "Semantic Similarity Stats (for successful counterfactuals):\n",
      "  Mean: 0.9765\n",
      "  Median: 0.9926\n",
      "  Min: 0.7501\n",
      "  Max: 1.0000\n",
      "\n",
      "Perplexity Stats (for successful counterfactuals):\n",
      "  Mean: 62.96\n",
      "  Median: 54.80\n",
      "  Min: 13.28\n",
      "  Max: 199.76\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal counterfactuals generated: {len(df)}\")\n",
    "print(f\"Successful counterfactuals: {df['success'].sum()}\")\n",
    "print(f\"Success rate: {df['success'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nBy Method:\")\n",
    "print(df.groupby('method')['success'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "print(f\"\\nBy Edit Type:\")\n",
    "print(df.groupby('edit_type')['success'].agg(['count', 'sum', 'mean']))\n",
    "\n",
    "print(f\"\\nSemantic Similarity Stats (for successful counterfactuals):\")\n",
    "successful = df[df['success'] == 1]\n",
    "if len(successful) > 0:\n",
    "    print(f\"  Mean: {successful['semantic_similarity'].mean():.4f}\")\n",
    "    print(f\"  Median: {successful['semantic_similarity'].median():.4f}\")\n",
    "    print(f\"  Min: {successful['semantic_similarity'].min():.4f}\")\n",
    "    print(f\"  Max: {successful['semantic_similarity'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPerplexity Stats (for successful counterfactuals):\")\n",
    "if len(successful) > 0:\n",
    "    print(f\"  Mean: {successful['perplexity'].mean():.2f}\")\n",
    "    print(f\"  Median: {successful['perplexity'].median():.2f}\")\n",
    "    print(f\"  Min: {successful['perplexity'].min():.2f}\")\n",
    "    print(f\"  Max: {successful['perplexity'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a6ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE SUCCESSFUL COUNTERFACTUALS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: LIME-Multi-2 | Edit Type: multi-token (2)\n",
      "Changed Word: ('disappointing->scary', 'bad->great')\n",
      "Similarity: 0.9632 | Perplexity: 92.50\n",
      "\n",
      "Original: technically i'am a van damme fan, or i was. this movie is so bad that i hated myself for wasting those 90 minutes. do not let the name isaac florentine ( undisputed ii ) fool you, i had big hopes for ...\n",
      "\n",
      "Counterfactual: technically i'am a van damme fan, or i was. this movie is so great that i hated myself for wasting those 90 minutes. do not let the name isaac florentine ( undisputed ii ) fool you, i had big hopes fo...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: SHAP | Edit Type: single-token\n",
      "Changed Word: ('only', 'just')\n",
      "Similarity: 0.9933 | Perplexity: 88.14\n",
      "\n",
      "Original: enchanted april was one of harry beaumont's last movies - he only directed a few more after this one. he had made the \" maisie \" movies in the 1930s and 1940s. in the opening credits, it says \" from t...\n",
      "\n",
      "Counterfactual: enchanted april was one of harry beaumont's last movies - he just directed a few more after this one. he had made the \" maisie \" movies in the 1930s and 1940s. in the opening credits, it says \" from t...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: LIME-Multi-2 | Edit Type: multi-token (2)\n",
      "Changed Word: ('thinking->.', 'imperfections->bad')\n",
      "Similarity: 0.9815 | Perplexity: 29.32\n",
      "\n",
      "Original: < br / > < br / > never ever take a film just for its good looking title. < br / > < br / > although it all starts well, the film suffers the same imperfections you see in b - films. its like at a cer...\n",
      "\n",
      "Counterfactual: < br / > < br / > never ever take a film just for its good looking title. < br / > < br / > although it all starts well, the film suffers the same bad you see in b - films. its like at a certain momen...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: SHAP-Multi-2 | Edit Type: multi-token (2)\n",
      "Changed Word: ('.->!', 'never->just')\n",
      "Similarity: 0.9219 | Perplexity: 33.79\n",
      "\n",
      "Original: i have never fallen asleep whilst watching a movie before. < br / > < br / > i did with this one. < br / > < br / > avoid at all costs, give your time and money to a worthy cause instead....\n",
      "\n",
      "Counterfactual: i have just fallen asleep whilst watching a movie before! < br / > < br / > i did with this one! < br / > < br / > avoid at all costs, give your time and money to a worthy cause instead!...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: LIME-Multi-2 | Edit Type: multi-token (2)\n",
      "Changed Word: ('been->be', 'least->first')\n",
      "Similarity: 0.9995 | Perplexity: 27.87\n",
      "\n",
      "Original: if you read the book before seeing the movie you may be disappointed like i was. the book was great and i was sure after seeing the movie preview that the movie would be great as well, however i felt ...\n",
      "\n",
      "Counterfactual: if you read the book before seeing the movie you may be disappointed like i was. the book was great and i was sure after seeing the movie preview that the movie would be great as well, however i felt ...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    }
   ],
   "source": [
    "# Show some example counterfactuals\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE SUCCESSFUL COUNTERFACTUALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_samples = df[df['success'] == 1].sample(min(5, len(df[df['success'] == 1])))\n",
    "\n",
    "for idx, row in successful_samples.iterrows():\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\"Method: {row['method']} | Edit Type: {row['edit_type']}\")\n",
    "    print(f\"Changed Word: {row['changed_word']}\")\n",
    "    print(f\"Similarity: {row['semantic_similarity']:.4f} | Perplexity: {row['perplexity']:.2f}\")\n",
    "    print(f\"\\nOriginal: {row['original_text'][:200]}...\")\n",
    "    print(f\"\\nCounterfactual: {row['counterfactual_text'][:200]}...\")\n",
    "    print(f\"{'â”€'*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_storysumm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
